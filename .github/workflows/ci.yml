name: CI
on:
  push:
  #   branches: [ master, main ]
  # pull_request:
  #   branches: [ master, main ]
jobs:
  Tests:
    timeout-minutes: 60
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [  macos-latest, ubuntu-latest,  ] # ubuntu-latest, windows-latest are currently non-functional, requiring adaptation for proper functionality. 
    steps:
      - name: update make
        shell: bash
        run: | 
          wget http://ftp.gnu.org/gnu/make/make-4.4.tar.gz
          tar xvf make-4.4.tar.gz
          cd make-4.4/
          ./configure
          make
          sudo make install

      - uses: actions/checkout@v3
        with:
          repository: jart/cosmopolitan
          path: './cosmopolitan'

      - name: support ape bins 1
        run: sudo cp ./cosmopolitan/build/bootstrap/ape.elf /usr/bin/ape

      - name: support ape bins 2
        run: sudo sh -c "echo ':APE:M::MZqFpD::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register"

      - name: make matrix
        run: cd ./cosmopolitan && V=0 make -j2 

      - name: Checkout Repository
        uses: actions/checkout@v4
    
      - name: Build and Install CosmoCC
        shell: bash
        run: |
          # mkdir -p cosmocc
          # cd cosmocc
          # curl -o cosmocc.zip -L  https://cosmo.zip/pub/cosmocc/cosmocc.zip
          # unzip cosmocc.zip
          # cd ..
          # ./cosmocc/bin/make -j8 && ./cosmocc/bin/make install
          make -j8 && make install
      - name: Create LLM Executable
        shell: bash 
        run: |
          curl -o mistral.gguf -L https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
          cat << EoF > .args
          -m
          mistral.gguf
          ...
          EoF
          cp /usr/local/bin/llamafile llamafile_exe
          chmod +x llamafile_exe
          zipalign -j0 \
            llamafile_exe \
            mistral.gguf \
            .args
      - name: Execute LLM CLI
        shell: bash
        run: |
          ./llamafile_exe --temp 0.7 --n-predict 50 -p '[INST]Write a story about llamas[/INST]'
